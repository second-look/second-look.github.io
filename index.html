<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="../../favicon.ico">

    <title>Second Look</title>

    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

    <!-- Optional theme -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

    <!-- Bootstrap core CSS -->
    <!-- <link href="../../dist/css/bootstrap.min.css" rel="stylesheet"> -->

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- Custom styles for this template -->
    <link href="starter-template.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <!-- <script src="../../assets/js/ie-emulation-modes-warning.js"></script> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <link rel="stylesheet" href="d3.slider.css" />

    <style>

       .page-header {
           border-bottom: 1px solid black
       }

       /*For jumbotron*/
       .jumbotron {
           margin-top: 20px;
           background-image: url("ribbon.jpeg");
           margin-bottom: 20px;
           min-height: 50%;
           height: 100vh;
           background-repeat: no-repeat;
           background-position: center;
           -webkit-background-size: cover;
           background-size: cover;
       }

    </style>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.2.1/jquery.min.js"></script>

    <!-- Latest compiled and minified JavaScript -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
</head>

<body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <!-- <a class="navbar-brand" href="#">Second Look</a> -->
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
          <ul class="nav navbar-nav">
            <li><a href="#">Home</a></li>
            <li><a href="#about">About</a></li>
            <li><a href="#problem">Problem</a></li>
            <li><a href="#value-proposition">Value Proposition</a></li>
            <li><a href="#solution">Solution</a></li>
            <li><a href="#demo">Demo</a></li>
            <li><a href="#results">Results</a></li>
            <li><a href="#future-improvements">Future Improvements</a></li>
            <li><a href="#team">Team Members</a></li>
          </ul>

        </div><!-- /.navbar-collapse -->
      </div><!-- /.container-fluid -->
    </nav>

    <!-- Main jumbotron for a primary marketing message or call to action -->
    <div class="jumbotron">
      <br> &nbsp;
      <div class="container">
        <h1 class="display-3">Second Look</h1>
        <p>AI-enabled breast cancer detection</p>
        <p><a class="btn btn-primary btn-lg" href="#about" role="button">Learn more &raquo;</a></p>
      </div>
    </div>

    <section class="primary" id="about">
        <div class="container">

            <div class="page-header">
                <h2>About</h2>
            </div>

            <p> Second Look is a computer aided detection (CAD) tool for radiologists specializing in mammography.  Enabled by object recognition and machine learning, the Second Look product provides radiologists with enhanced decision support, resulting in better outcomes for patients.

        </div>

    </section>

    <section class="primary" id="problem">
        <div class="container">

            <div class="page-header">
                <h2>Problem</h2>
            </div>
              <p>Breast cancer is the most prevalent type of cancer among women across the world.  In 2012, 1.677 million new breast cancer cases were diagnosed worldwide, representing <a href= "http://www.wcrf.org/int/cancer-facts-figures/worldwide-data" target="_blank"> 25.2% of all cancer cases in women</a>.  In 2011, <a href = "http://www.who.int/cancer/detection/breastcancer/en/index1.html" target="_blank">  508,000 women died due to breast cancer</a>.  In the United States alone, there are an estimated <a href="https://www.cancer.org/content/dam/cancer-org/research/cancer-facts-and-statistics/breast-cancer-facts-and-figures/breast-cancer-facts-and-figures-2017-2018.pdf" target="_blank"> 316,120 new cases of breast cancer and 40,610 deaths in 2017</a>.</p>
              <p>In the United States, breast cancer incidence rates increased by <a href= "https://www.cancer.org/content/dam/cancer-org/research/cancer-facts-and-statistics/breast-cancer-facts-and-figures/breast-cancer-facts-and-figures-2017-2018.pdf" target="_blank"> 10-50% across various ethnicities during 1975-2014</a>. However, during the same period, breast cancer mortality rates decreased by <a href = "https://www.cancer.org/content/dam/cancer-org/research/cancer-facts-and-statistics/breast-cancer-facts-and-figures/breast-cancer-facts-and-figures-2017-2018.pdf" target="_blank"> 5-30% across various ethnicities</a>.
              Early detection of breast cancer leading to early treatment has contributed to a <a href= "http://www.nejm.org/doi/pdf/10.1056/NEJMoa050518" target="_blank"> reduction in breast cancer mortality rates</a>. Screening mammography is one of the most common tools used for early detection of breast cancer. Screening mammography has become significantly more commonplace in the United States during the <a href= "https://www.cdc.gov/nchs/data/hus/hus16.pdf#070" target="_blank"> last 30 years (1987-2017)</a>. However, screening mammography has a <a href = "http://www.bcsc-research.org/statistics/performance/screening/2009/perf_age.html" target="_blank"> sensitivity of 84.4%</a>. This means that 15.6% of the breast cancer cases remain undetected.</p>

              <center>
                       <img src="Breast Density.png" class="img-responsive img-square" style="width: 515px; height: 180px">
                       <p><b>Figure 1:</b> Breast density in mammograms</p>
                       <p> <b>Source:</b><a href = "https://ww5.komen.org/Breastcancer/Highbreastdensityonmammogram.html" target="_blank">
                        Susan G. Komen Foundation </a></p>
            </center>

            <p>As seen in Figure 1, fatty tissue appears darker on a mammogram and fibroglandular tissue appears light gray or white on a mammogram. Breast tumors also appear light gray or white on a mammogram.</p>

            <center>
                       <img src="Breast Density 1.png" class="img-responsive img-square" style="width: 515px; height: 220px">
                       <p><b>Figure 2:</b> Breast tumor in fatty and dense breast</p>
                       <p> <b>Source:</b><a href = "https://www.today.com/health/do-you-have-dense-breasts-you-may-need-more-mammogram-t104102" target="_blank">
                        Do you need extra screening for breast cancer? </a></p>
            </center>
            <p>As seen in Figure 2, detecting breast tumor in dense tissue is difficult compared to detecting breast tumor in fatty tissue.</p>


            <p>The sensitivity of breast tumor detection is 87% in almost entirely fatty breast, while it is only <a href="http://annals.org/aim/article-abstract/716007/individual-combined-effects-age-breast-density-hormone-replacement-therapy-use?doi=10.7326/0003-4819-138-3-200302040-00008" target="_blank">62.9% in extremely dense breasts</a>.  Women with dense breasts have a higher risk of <a href="http://ascopubs.org/doi/abs/10.1200/jco.2009.26.4770" target="_blank">developing breast cancer</a>.  This combination of higher cancer risk with lower detection rates is concerning for women with dense breasts.</p>

            <p>In addition, considerable differences were observed in the interpretation of same mammograms by several radiologists as well as the interpretation of same mammograms <a href="https://academic.oup.com/jnci/article/90/23/1801/2520483#43132835" target="_blank">by the same radiologist</a>. So using multiple radiologists to improve breast cancer detection accuracy is time consuming, costly, and may not improve outcomes in some cases.</p>

            <p>A software tool that detects breast tumors more accurately and more consistently would be beneficial to radiologists. Further, recent advances in deep learning algorithms for image classification, object localization, object recognition, semantic segmentation and instance segmentation make them attractive candidates for the development of a software tool that detects breast cancer in mammograms.</p>


        </div>

    </section>

    <section class="primary" id="value-proposition">
        <div class="container">
          <div class="page-header">
              <h2>Value Proposition</h2>
          </div>

          <div class="row text-center">
               <div class="col-md-6">
                   <center>
                     <img src="target.png" class="img-responsive img-square" style="width: 174px; height: 174px">
                   </center>
                     <h4 class="service-heading">Increased sensitivity</h4>
                     <p class="text-muted">Results in earlier detection of breast cancer cases.  Research demonstrates that individuals with earlier detection have a higher survival rate.</p>
               </div>
               <div class="col-md-6">
                   <center>
                     <img src="opinion.png" class="img-responsive img-square" style="width: 174px; height: 174px">
                   </center>
                   <h4 class="service-heading">Automated second opinion</h4>
                   <p class="text-muted">Provides a "second look" after the radiologist has reviewed the mammogram.</p>
               </div>
          </div>

          <div class="row text-center">
               <div class="col-md-6">
                   <center>
                     <img src="global.png" class="img-responsive img-square" style="width: 174px; height: 174px">
                   </center>
                   <h4 class="service-heading">Scalable to the global radiologist population</h4>
                   <p class="text-muted">Broad reach to radiologists in both developed and developing countries, benefitting countries with a shortage of radiologists.</p>
               </div>
               <div class="col-md-6">
                   <center>
                     <img src="workflow.png" class="img-responsive img-square" style="width: 174px; height: 174px">
                   </center>
                   <h4 class="service-heading">Easily integrated into the workflow of radiologists</h4>
                   <p class="text-muted">Designed with feedback from radiologists.  Highlights the specific region of the abnormalities. Easy-to-use user interface.</p>
               </div>
          </div>
        </div>
    </section>

    <section id="solution">
        <div class="container">
            <div class="page-header">
                <h2>Solution</h2>
            </div>

            <h3> Approach: Instance Segmentation </h3>
            The majority of deep learning papers have focused on the breast tumor classification problem as an “Image Classification” problem. While classifying a mammogram as “normal”, “benign”, or “malignant” is necessary, it is not sufficient to help a radiologist identify hard-to-detect tumors. Further, a mammogram image may contain multiple tumors of different types (benign/malignant). Such mammograms clearly warrant an “Instance Segmentation” approach for accurate tumor detection.
            <br> &nbsp;
            <div>
            <center>
                       <img src="Instance Segmentation.png" class="img-responsive img-square" style="width: 600px; height: 300px">
                       <p> <b>Source:</b><a href = "https://medium.com/@nikasa1889/the-modern-history-of-object-recognition-infographic-aea18517c318" target="_blank">
                        The Modern History of Object Recognition - Infographic </a></p>
            </center>
            </div>
            <br>

            <h3>Architecture: Mask R-CNN </h3>
            <p> He et al. created <a href = "https://arxiv.org/pdf/1703.06870.pdf" target="_blank"> Mask R-CNN </a> by extending <a href="https://arxiv.org/pdf/1506.01497.pdf" target="_blank"> Faster R-CNN</a> by adding a branch for predicting class-specific object mask for Instance Segmentation in parallel with the existing object classifier and bounding box regressor.
            <div>
            <center>
                       <img src="Mask RCNN.png" class="img-responsive img-square" style="width: 600px; height: 300px">
                       <p> <b>Source:</b><a href = "https://arxiv.org/pdf/1703.06870.pdf" target="_blank">
                        Mask R-CNN </a></p>
            </center>
            </div>
            <br>

            <!-- Mask R-CNN has two main features on top of Faster R-CNN:
            <ol>
              <li>RoiPooling is changed to RoiAlign</li>
              <li>It achieves pixel-level segmentation by adding a branch to Faster R-CNN that outputs a binary mask for each class. The branch is achieved by using a Fully Convolutional Network on top of a CNN based feature map. </li>
            </ol> -->
            RoIPooling layer of Faster R-CNN is replaced by RoIAlign layer which uses bilinear interpolation to calculate a segmentation mask at a pixel by pixel level.
            </p>
            <br>
            <div>
            <center>
                       <img src="Mask RCNN Explained.png" class="img-responsive img-square" style="width: 500px; height: 400px">
                       <p> <b>Source:</b><a href = "https://arxiv.org/pdf/1703.06870.pdf" target="_blank">
                        Mask R-CNN Architecture</a></p>
            </center>
            </div>
            <br>

            <h3>Data Preprocessing</h3>
              <br>
              <h4> 1. Dataset Selection </h4>
                <p>The <a href="https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM" target="_blank"> CBIS-DDSM </a>dataset has four sub datasets: Mass-Training, Mass-Test, Calc-Training and Calc-Test. Mass-Training has images for 1318 tumors. Mass-Test has images for 378 tumors. Calc-Training has images for 1622 calcifications. Calc-Test has images for 326 calcifications. For this project, we used the Mass-Training and Mass-Test datasets only.</p>
                <p>The Mass-Training dataset has images for 637 'Malignant' tumors, 577 'Benign' tumors, and 104 'Benign without callback' tumors. We combined 'Benign' and 'Benign without callback' into a single 'Benign' class which has images for 681 tumors. All of them have corresponding segmentation masks available as the ground truth.</p>
                <p>The Mass-Test dataset has images for 147 'Malignant' tumors, 194 'Benign' tumors, and 37 'Benign without callback' tumors. We combined 'Benign' and 'Benign without callback' into a single 'Benign' class which has images for 231 tumors. All of them have corresponding segmentation masks available as the ground truth.</p>
                <p><a href="http://peipa.essex.ac.uk/info/mias.html" target="_blank">The mini-MIAS dataset</a> has images for 209 'No tumor' and 121 'Tumor' cases. Since the tumor cases do not have corresponding segmentation masks available, we did not use the 121 'Tumor' cases. We randomly split the 209 'No tumor' cases into 120 for training and 89 for testing.</p>
              <br>
              <h4> 2. Data Augmentation </h4>
                <p>The combined CBIS-DDSM Mass-Train (1318 cases) + mini-MIAS training (120 cases) is a small dataset. We applied data augmentation to it. We applied 6 rotations (100, 200, 300, -100, -200, -300 ) and created a total of CBIS-DDSM Mass-Train (9226 cases) + mini-MIAS training (840 cases). Then we flipped all of those images horizontally and created a total of CBIS-DDSM Mass-Train (18,452 cases) + mini-MIAS training (1680 cases).</p>
                <p>The combined CBIS-DDSM Mass-Test (378 cases) + mini- MIAS test (98 cases) is a small dataset. We applied data augmentation to it. We applied 6 rotations (100, 200, 300, -100, -200, -300 ) and created a total of CBIS-DDSM Mass-Test (2646 cases) + mini-MIAS test (686 cases). Then we flipped all of those images horizontally and created a total of CBIS- DDSM Mass-Test (5292 cases) + mini-MIAS test (1372 cases).</p>
                <p>During mammogram acquisition, the technicians ensure that breasts are properly positioned when acquiring <a href="http://www.mammoguide.com/mammo-techniques.html" target="_blank">craniocaudal (CC) and mediolateral oblique (MLO) views</a>. This ensures that the nipples are close to the center of the image. Hence, we chose not to apply translations to mammograms as it is highly unlikely to have a mammogram with the nipple farther away from the image center. </p>
              <br>
              <h4> 3. Patch Extraction </h4>
                <p> The CBIS-DDSM Mass-Train (18,452 cases) dataset has image sizes varying from 2000 x 2000 to 5000 x 5000 pixels. The mini-MIAS training (1680 cases) dataset has all images with size 1024 x 1024 pixels. The smallest tumor in CBIS-DDSM is 54 x 85 pixels. The Mask R-CNN can accept 256 x 256 images without resizing. A 2k x 2k image resized to 256 x 256 will result in 8 times reduction in width/height and 64 times reduction in area. It will be 20 times reduction in width/height and 400 times reduction in area. Small tumors will shrink so much with such reductions, that it will become impossible for the network to learn their morphology. Since, small tumors are also harder to detect for the radiologists, it is necessary that the network can detect small tumors with high accuracy.</p>
                <p>Hence, instead of using the whole images, we extracted patches of size 256 x 256. Many images do not have widths and heights that are a multiple of 256. So we zero padded them at right and bottom to increase their widths and heights appropriately before extracting patches.</p>
                <p>Over 3 million 256 x 256 pixel patches were extracted from the CBIS-DDSM Mass-Train (18,452 cases) images. 26,880 patches were extracted from the mini-MIAS training (1680 cases) images. Unlike the original images that had tumors, surrounding breast tissue and black background in each mammogram; a majority of the patches had only the black background, only the breast tissue without tumor or a combination of the black background with the breast tissue without tumor. About 250,000+ patches had either just tumor or tumor with surrounding breast tissue and the black background.</p>
                <p>We kept all 250,000 patches with breast tumor and sampled 300,000 patches that had 100,000 black background, 100,000 breast tissue, and 100,000 black background with breast tissue. We used this dataset to train Mask R-CNN.</p>
                <p>The CBIS-DDSM Mass-Test (5292 cases) dataset has image sizes varying from 2000 x 2000 to 5000 x 5000 pixels. The mini-MIAS test (1327 cases) dataset has all images with size 1024 x 1024 pixels.</p>
                <p>846,720 256 x 256 pixel patches were extracted from the CBIS-DDSM Mass-Test (5292 cases) images. 21,952 patches were extracted from the mini-MIAS training (1372 cases) images. We used all of those patches for testing.</p>
            <br>

            <h3>Network Architecture</h3>
            <p>We derived our Mask R-CNN architecture based on the <a href="https://arxiv.org/abs/1612.03144" target="_blank"> Functional Pyramid Network (FPN)</a> variant of Mask R-CNN.</p>
            <p>Feature Pyramid Network extracts features at different scales based on their levels in the feature pyramid. <a href="https://arxiv.org/abs/1512.03385" target="_blank">The ResNet</a> – <a href="https://arxiv.org/abs/1612.03144" target="_blank">FPN</a> backbone provides good accuracy without sacrificing speed.</p>
            <p>Our Mask R-CNN architecture leverages and extends the framework of Abdullah et al. at <a href="https://github.com/matterport/Mask_RCNN" target="_blank">Matterport</a>. Our Mask R-CNN model has – 63,744,170 total parameters, 63,632,682 trainable parameters, and 111,488 non-trainable parameters.</p>
            <br>

            <h3>Training & Testing</h3>
            <p>We used Google Cloud Platform virtual machines for training and testing. Our virtual machine environment had the following specifications:</p>
                <ol type='1'>
                <li>Zone: us-west1-b </li>
                <li>Machine type: n1-standard-4</li>
                <li>CPUs: 4 vCPUs (unknown CPU platform)</li>
                <li>CPU RAM: 15 GB</li>
                <li>GPU: 1 Nvidia Tesla K80 (half board)</li>
                <li>GPU RAM: 12 GB</li>
                <li>HDD: 1 TB persistent storage</li>
                <li>OS: Ubuntu 16.04.3 LTS</li>
                <li>Nvidia CUDA: CUDA 8.0.61</li>
                <li>Nvidia cuDNN: cuDNN 6.0.21</li>
                <li>Google Tensorflow: TensorFlow 1.3.0</li>
                <li>Keras: Keras 2.0.8</li>
                <li>OpenCV: OpenCV 3.3.0</li>
                <li>Jupyter Notebook: Jupyter Notebook 5.1.0</li></ol>
                <p>On Google Cloud Platform, Tesla P100 GPUs cost 3 times Tesla K80 GPUs. However, while using Tesla P100 GPUs, there was only a 30-40% performance increase over Tesla K89 GPUs. Hence, we used K80 GPUs for the bulk of our training and testing activities as they had a better performance/price ratio.
                We used Mask-RCNN pretrained on the <a href="http://cocodataset.org/#home" target="_blank">COCO dataset</a>.</p>
                <p>We observed that Mask-RCNN requires a large amount of memory and hence we could only train 2 images in a batch per GPU in the available environment reliably without experiencing memory errors. This small batch size was equivalent to stochastic training instead of a mini-batch training. To maintain the network stability we were forced to use a lower learning rate of 0.002 which we had to reduce in later epochs to 0.0002.</p>

        </div>
    </section>

    <section id="demo">
        <div class="container">
            <div class="page-header">
                <h2>Demo</h2>
            </div>

            <p> The video below shows the Second Look user interface: </p>

            <video width="854" height="480" controls>
              <source src="MVP Demo v2.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video>

            <br> &nbsp;
            <p>
              To see a demo of the solution, please follow these steps:
              <ol>
                <li> Build and launch the repository by clicking this <a href="https://mybinder.org/v2/gh/jlandesman/mammoGAN.git/master" target="_self">link</a>.</li>
                <li> Navigate to the "inference" folder.</li>
                <li> Open "ROIBreastTumorsDataset-Patches-Demo.ipynb"</li>
                <li> Execute code from start to the "Ground Truth" section once.</li>
                <li> The "Ground Truth" and "Prediction" sections can be executed multiple times - every time the code will choose a random image, and show the ground truth and prediction.
              </ol>
            </p>
        </div>
    </section>

    <section id="results">
        <div class="container">
            <div class="page-header">
                <h2>Results</h2>
            </div>

            <p>
              When we split images into patches, we observed that many
              tumors got split unevenly into two or more patches where one
              or two patches contained the majority of the tumor portions
              and the remaining patches contained very small tumor portions.
              Those very small tumor portions did not seem to have any
              characteristic features of tumors (e.g. microcalcifications,
              mass morphology etc.) for Mask R-CNN to identify them
              as parts of the tumors, just based on the small portion in a
              single patch alone. We expected Mask R-CNN to fail to
              identify those small portions of tumors and observed the same.
            </p>

            <p>
              Hence, we calculated prediction accuracy at both the patch
              level and image level. Let's look at an example. An image
              containing a tumor is split into 4 patches, where one patch
              contains the majority of the tumor, another part contains a small
              portion of the tumor and the remaining two patches contain no
              tumor.  Mask R-CNN identifies the patch with the
              majority of the tumor as “tumor”, the patch with the small
              portion of the tumor as “no tumor” and the two patches with
              “no tumor” as “no tumor”. The patch level accuracy = 3
              correct predictions out of 4 predictions = 0.75. When we
              consider the whole image, one patch that contained the
              majority of the tumor was correctly identified by Mask RCNN
              as “tumor”. Hence the image level accuracy = 1 correct
              prediction out of 1 total predictions = 1.0.
            </p>

            <p>
              Using the methodology described above, we calculated
              prediction accuracy and the area under the receiver operating
              characteristic curve for patches and images.
            </p>

              <table class="table">
                <thead>
                  <tr>
                    <th scope="col">No.</th>
                    <th scope="col">Type</th>
                    <th scope="col">Accuracy</th>
                    <th scope="col">AUC</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th scope="row">1</th>
                    <td>Patches</td>
                    <td>0.83</td>
                    <td>0.73</td>
                  </tr>
                  <tr>
                    <th scope="row">2</th>
                    <td>Images</td>
                    <td>1</td>
                    <td>0.93</td>
                  </tr>
                </tbody>
              </table>

            <div class="row">
              <div class="col-md-6">
                <center>
                  <img src="patches-roc.png" class="img-responsive img-square" style="width: 515px">
                </center>
              </div>
              <div class="col-md-6">
                <center>
                  <img src="images-roc.png" class="img-responsive img-square" style="width: 515px">
                </center>
              </div>
            </div>

        </div>
    </section>

    <section class="primary" id="future-improvements">
        <div class="container">

            <div class="page-header">
                <h2>Future Improvements</h2>
            </div>

            <ol>
              <li>Investigate false negatives and false positives </li>
              <li>Augment existing dataset with more training examples</li>
              <li>Gather feedback from more radiologists specializing in mammography</li>
            </ol>

        </div>

    </section>

    <section id="team">

        <div class="container">
          <div class="page-header">
              <h2>Team Members</h2>
          </div>

          <div class="row">

        	  <div class="col-sm-4" style="width: 290px;">
              <figure>
                <center>
                  <a href="https://www.linkedin.com/in/abhijit-thatte/">
          	         <img src="abhijit.jpg" class="img-thumbnail" alt="Team Member" style="width: 174px; height: 174px">
                  </a>
                  <figcaption><b>Abhijit Thatte</b></figcaption>
                </center>
        	    </figure>
        	  </div>

      		  <div class="col-sm-4" style="width: 290px;">
      		    <figure>
                <center>
                  <a href="https://www.linkedin.com/in/andrewylam/">
        		          <img src="andrew.png" class="img-thumbnail" alt="Team Member" style="width: 174px; height: 174px">
                  </a>
                  <figcaption><b>Andrew Lam</b></figcaption>
                </center>
      		    </figure>
      		  </div>

      		  <div class="col-sm-4" style="width: 290px;">
      		    <figure>
                <center>
                  <a href="https://www.linkedin.com/in/ankithgunapal/">
        		          <img src="ankith.jpg" class="img-thumbnail" alt="Team Member" style="width: 174px; height: 174px">
                  </a>
                  <figcaption><b>Ankith Gunapal</b></figcaption>
                </center>
      		    </figure>
      		  </div>

            <div class="col-sm-4" style="width: 290px;">
              <figure>
                <center>
                  <a href="https://www.linkedin.com/in/jlandesman/">
                    <img src="jonathan.jpg" class="img-thumbnail" alt="Team Member" style="width: 174px; height: 174px">
                  </a>
                  <figcaption><b>Jonathan Landesman</b></figcaption>
                </center>
              </figure>
            </div>

          </div>
	      </div>
     </section>
<script>
     // Select all links with hashes
$('a[href*="#"]')
  // Remove links that don't actually link to anything
  .not('[href="#"]')
  .not('[href="#0"]')
  .click(function(event) {
    // On-page links
    if (
      location.pathname.replace(/^\//, '') == this.pathname.replace(/^\//, '')
      &&
      location.hostname == this.hostname
    ) {
      // Figure out element to scroll to
      var target = $(this.hash);
      target = target.length ? target : $('[name=' + this.hash.slice(1) + ']');
      // Does a scroll target exist?
      if (target.length) {
        // Only prevent default if animation is actually gonna happen
        event.preventDefault();
        $('html, body').animate({
          scrollTop: target.offset().top
        }, 1000, function() {
          // Callback after animation
          // Must change focus!
          var $target = $(target);
          $target.focus();
          if ($target.is(":focus")) { // Checking if the target was focused
            return false;
          } else {
            //$target.attr('tabindex','-1'); // Adding tabindex for elements not focusable
            $target.focus(); // Set focus again
          };
        });
      }
    }
  });
  </script>

</body>

</html>
